def group(*choices): return '(' + '|'.join(choices) + ')'
    """
    '('
    """
def any(*choices): return group(*choices) + '*'
    """
    '*'
    """
def maybe(*choices): return group(*choices) + '?'
    """
    '?'
    """
def _combinations(*l):
    """

    """
def TokenError(Exception): pass
    """
     for testing
    """
def tokenize(readline, tokeneater=printtoken):
    """

        The tokenize() function accepts two parameters: one representing the
        input stream, and one providing an output mechanism for tokenize().

        The first parameter, readline, must be a callable object which provides
        the same interface as the readline() method of built-in file objects.
        Each call to the function should return one line of input as a string.

        The second parameter, tokeneater, must also be a callable object. It is
        called once for each token, with five arguments, corresponding to the
        tuples generated by generate_tokens().
    
    """
def tokenize_loop(readline, tokeneater):
    """
 
    """
    def untokenize(self, iterable):
        """

        """
    def compat(self, token, iterable):
        """
        ' '
        """
def _get_normal_name(orig_enc):
    """
    Imitates get_normal_name in tokenizer.c.
    """
def detect_encoding(readline):
    """

        The detect_encoding() function is used to detect the encoding that should
        be used to decode a Python source file. It requires one argument, readline,
        in the same way as the tokenize() generator.

        It will call readline a maximum of twice, and return the encoding used
        (as a string) and a list of any lines (left as bytes) it has read
        in.

        It detects the encoding from the presence of a utf-8 bom or an encoding
        cookie as specified in pep-0263. If both a bom and a cookie are present, but
        disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
        charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
        'utf-8-sig' is returned.

        If no encoding is specified, then the default of 'utf-8' will be returned.
    
    """
    def read_or_stop():
        """
        'ascii'
        """
def untokenize(iterable):
    """
    Transform tokens back into Python source code.

        Each element returned by the iterable must be a token sequence
        with at least two elements, a token number and token value.  If
        only two tokens are passed, the resulting output is poor.

        Round-trip invariant for full input:
            Untokenized source will match input source exactly

        Round-trip invariant for limited input:
            # Output text will tokenize the back to the input
            t1 = [tok[:2] for tok in generate_tokens(f.readline)]
            newcode = untokenize(t1)
            readline = iter(newcode.splitlines(1)).next
            t2 = [tok[:2] for tokin generate_tokens(readline)]
            assert t1 == t2
    
    """
def generate_tokens(readline):
    """

        The generate_tokens() generator requires one argument, readline, which
        must be a callable object which provides the same interface as the
        readline() method of built-in file objects. Each call to the function
        should return one line of input as a string.  Alternately, readline
        can be a callable function terminating with StopIteration:
            readline = open(myfile).next    # Example of alternate readline

        The generator produces 5-tuples with these members: the token type; the
        token string; a 2-tuple (srow, scol) of ints specifying the row and
        column where the token begins in the source; a 2-tuple (erow, ecol) of
        ints specifying the row and column where the token ends in the source;
        and the line on which the token was found. The line passed is the
        physical line.
    
    """
