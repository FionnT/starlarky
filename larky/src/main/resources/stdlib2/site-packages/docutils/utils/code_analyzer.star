    def ResourceError(ApplicationError):
    """
     stub
    """
def LexerError(ApplicationError):
    """
    Parse `code` lines and yield "classified" tokens.

        Arguments

          code       -- string of source code to parse,
          language   -- formal language the code is written in,
          tokennames -- either 'long', 'short', or '' (see below).

        Merge subsequent tokens of the same token-type.

        Iterating over an instance yields the tokens as ``(tokentype, value)``
        tuples. The value of `tokennames` configures the naming of the tokentype:

          'long':  downcased full token type name,
          'short': short name defined by pygments.token.STANDARD_TYPES
                   (= class argument used in pygments html output),
          'none':      skip lexical analysis.
    
    """
    def __init__(self, code, language, tokennames='short'):
        """

                Set up a lexical analyzer for `code` in `language`.
        
        """
    def merge(self, tokens):
        """
        Merge subsequent tokens of same token-type.

                   Also strip the final newline (added by pygments).
        
        """
    def __iter__(self):
        """
        Parse self.code and yield "classified" tokens.
        
        """
def NumberLines(object):
    """
    Insert linenumber-tokens at the start of every code line.

        Arguments

           tokens    -- iterable of ``(classes, value)`` tuples
           startline -- first line number
           endline   -- last line number

        Iterating over an instance yields the tokens with a
        ``(['ln'], '<the line number>')`` token added for every code line.
        Multi-line tokens are splitted.
    """
    def __init__(self, tokens, startline, endline):
        """
         pad linenumbers, e.g. endline == 100 -> fmt_str = '%3d '

        """
    def __iter__(self):
        """
        'ln'
        """
