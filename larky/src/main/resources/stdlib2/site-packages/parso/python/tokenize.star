    def is_identifier(s):
        """
        'capture'
        """
def maybe(*choices):
    """
    '?'
    """
def _all_string_prefixes(version_info, include_fstring=False, only_fstring=False):
    """
    ''
    """
def _compile(expr):
    """
    r'(?:\{\{|\}\}|\\(?:\r\n?|\n)|[^{}\r\n])+'
    """
def _create_token_collection(version_info):
    """
     Note: we use unicode matching for names ("\w") but ascii matching for
     number literals.

    """
def Token(namedtuple('Token', ['type', 'string', 'start_pos', 'prefix'])):
    """
    'TokenInfo(type=%s, string=%r, start_pos=%r, prefix=%r)'
    """
def FStringNode(object):
    """
    ''
    """
    def open_parentheses(self, character):
        """
         No parentheses means that the format spec is also finished.

        """
    def allow_multiline(self):
        """
        ''
        """
def _find_fstring_string(endpats, fstring_stack, line, lnum, pos):
    """
     even if allow_multiline is False, we still need to check for trailing
     newlines, because a single-line f-string can contain line continuations

    """
def tokenize(code, version_info, start_pos=(1, 0)):
    """
    Generate tokens from a the source code (string).
    """
def _print_tokens(func):
    """

        A small helper function to help debug the tokenize_lines function.
    
    """
    def wrapper(*args, **kwargs):
        """
         This print is intentional for debugging!
        """
def tokenize_lines(lines, version_info, start_pos=(1, 0), indents=None, is_first_token=True):
    """

        A heavily modified Python standard library tokenizer.

        Additionally to the default information, yields also the prefix of each
        token. This idea comes from lib2to3. The prefix contains all information
        that is irrelevant for the parser like newlines in parentheses or comments.
    
    """
    def dedent_if_necessary(start):
        """
        ''
        """
def _split_illegal_unicode_name(token, start_pos, prefix):
    """
    ''
    """
