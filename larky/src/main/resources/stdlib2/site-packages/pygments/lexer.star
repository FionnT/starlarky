def LexerMeta(type):
    """

        This metaclass automagically converts ``analyse_text`` methods into
        static methods which always return float values.
    
    """
    def __new__(mcs, name, bases, d):
        """
        'analyse_text'
        """
def Lexer(metadef=LexerMeta):
    """

        Lexer for a specific language.

        Basic options recognized:
        ``stripnl``
            Strip leading and trailing newlines from the input (default: True).
        ``stripall``
            Strip all leading and trailing whitespace from the input
            (default: False).
        ``ensurenl``
            Make sure that the input ends with a newline (default: True).  This
            is required for some lexers that consume input linewise.

            .. versionadded:: 1.3

        ``tabsize``
            If given and greater than 0, expand tabs in the input (default: 0).
        ``encoding``
            If given, must be an encoding name. This encoding will be used to
            convert the input string to Unicode, if it is not already a Unicode
            string (default: ``'guess'``, which uses a simple UTF-8 / Locale /
            Latin1 detection.  Can also be ``'chardet'`` to use the chardet
            library, if it is installed.
        ``inencoding``
            Overrides the ``encoding`` if given.
    
    """
    def __init__(self, **options):
        """
        'stripnl'
        """
    def __repr__(self):
        """
        '<pygments.lexers.%s with %r>'
        """
    def add_filter(self, filter_, **options):
        """

                Add a new stream filter to this lexer.
        
        """
    def analyse_text(text):
        """

                Has to return a float between ``0`` and ``1`` that indicates
                if a lexer wants to highlight this text. Used by ``guess_lexer``.
                If this method returns ``0`` it won't highlight it in any case, if
                it returns ``1`` highlighting with this lexer is guaranteed.

                The `LexerMeta` metaclass automatically wraps this function so
                that it works like a static method (no ``self`` or ``cls``
                parameter) and the return value is automatically converted to
                `float`. If the return value is an object that is boolean `False`
                it's the same as if the return values was ``0.0``.
        
        """
    def get_tokens(self, text, unfiltered=False):
        """

                Return an iterable of (tokentype, value) pairs generated from
                `text`. If `unfiltered` is set to `True`, the filtering mechanism
                is bypassed even if filters are defined.

                Also preprocess the text, i.e. expand tabs and strip it if
                wanted and applies registered filters.
        
        """
        def streamer():
            """

                    Return an iterable of (index, tokentype, value) pairs where "index"
                    is the starting position of the token within the input text.

                    In subclasses, implement this method as a generator to
                    maximize effectiveness.
        
            """
def DelegatingLexer(Lexer):
    """

        This lexer takes two lexer as arguments. A root lexer and
        a language lexer. First everything is scanned using the language
        lexer, afterwards all ``Other`` tokens are lexed using the root
        lexer.

        The lexers from the ``template`` lexer package use this base lexer.
    
    """
    def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
        """
        ''
        """
def include(str):  # pylint: disable=invalid-name
    """
     pylint: disable=invalid-name
    """
def _inherit:
    """

        Indicates the a state should inherit from its superclass.
    
    """
    def __repr__(self):
        """
        'inherit'
        """
def combined(tuple):  # pylint: disable=invalid-name
    """
     pylint: disable=invalid-name
    """
    def __new__(cls, *args):
        """
         tuple.__init__ doesn't do anything

        """
def _PseudoMatch:
    """

        A pseudo match object constructed from a string.
    
    """
    def __init__(self, start, text):
        """
        'No such group'
        """
    def groups(self):
        """

            Callback that yields multiple actions for each group in the match.
    
        """
    def callback(lexer, match, ctx=None):
        """

            Special singleton used for indicating the caller class.
            Used by ``using``.
    
        """
def using(_other, **kwargs):
    """

        Callback that processes the match with a different lexer.

        The keyword arguments are forwarded to the lexer, except `state` which
        is handled separately.

        `state` specifies the state that the new lexer will start in, and can
        be an enumerable such as ('root', 'inline', 'string') or a simple
        string which is assumed to be on top of the root state.

        Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.
    
    """
        def callback(lexer, match, ctx=None):
            """
             if keyword arguments are given the callback
             function has to create a new lexer instance

            """
        def callback(lexer, match, ctx=None):
            """
             XXX: cache that somehow

            """
def default:
    """

        Indicates a state or state action (e.g. #pop) to apply.
        For example default('#pop') is equivalent to ('', Token, '#pop')
        Note that state tuples may be used as well.

        .. versionadded:: 2.0
    
    """
    def __init__(self, state):
        """

            Indicates a list of literal words that is transformed into an optimized
            regex that matches any of the words.

            .. versionadded:: 2.0
    
        """
    def __init__(self, words, prefix='', suffix=''):
        """

            Metaclass for RegexLexer, creates the self._tokens attribute from
            self.tokens on the first instantiation.
    
        """
    def _process_regex(cls, regex, rflags, state):
        """
        Preprocess the regular expression component of a token definition.
        """
    def _process_token(cls, token):
        """
        Preprocess the token component of a token definition.
        """
    def _process_new_state(cls, new_state, unprocessed, processed):
        """
        Preprocess the state transition action of a token definition.
        """
    def _process_state(cls, unprocessed, processed, state):
        """
        Preprocess a single state definition.
        """
    def process_tokendef(cls, name, tokendefs=None):
        """
        Preprocess a dictionary of token definitions.
        """
    def get_tokendefs(cls):
        """

                Merge tokens from superclasses in MRO order, returning a single tokendef
                dictionary.

                Any state that is not defined by a subclass will be inherited
                automatically.  States that *are* defined by subclasses will, by
                default, override that state in the superclass.  If a subclass wishes to
                inherit definitions from a superclass, it can use the special value
                "inherit", which will cause the superclass' state definition to be
                included at that point in the state.
        
        """
    def __call__(cls, *args, **kwds):
        """
        Instantiate cls after preprocessing its token definitions.
        """
def RegexLexer(Lexer, metadef=RegexLexerMeta):
    """

        Base for simple stateful regular expression-based lexers.
        Simplifies the lexing process so that you need only
        provide a list of states and regular expressions.
    
    """
    def get_tokens_unprocessed(self, text, stack=('root',)):
        """

                Split ``text`` into (tokentype, text) pairs.

                ``stack`` is the inital stack (default: ``['root']``)
        
        """
def LexerContext:
    """

        A helper object that holds lexer position data.
    
    """
    def __init__(self, text, pos, stack=None, end=None):
        """
         end=0 not supported ;-)
        """
    def __repr__(self):
        """
        'LexerContext(%r, %r, %r)'
        """
def ExtendedRegexLexer(RegexLexer):
    """

        A RegexLexer that uses a context object to store its state.
    
    """
    def get_tokens_unprocessed(self, text=None, context=None):
        """

                Split ``text`` into (tokentype, text) pairs.
                If ``context`` is given, use this lexer context instead.
        
        """
def do_insertions(insertions, tokens):
    """

        Helper for lexers which must combine the results of several
        sublexers.

        ``insertions`` is a list of ``(index, itokens)`` pairs.
        Each ``itokens`` iterable should be inserted at position
        ``index`` into the token stream given by the ``tokens``
        argument.

        The result is a combined token stream.

        TODO: clean up the code here.
    
    """
def ProfilingRegexLexerMeta(RegexLexerMeta):
    """
    Metaclass for ProfilingRegexLexer, collects regex timing info.
    """
    def _process_regex(cls, regex, rflags, state):
        """
        Drop-in replacement for RegexLexer that does profiling of its regexes.
        """
    def get_tokens_unprocessed(self, text, stack=('root',)):
        """
         this needs to be a stack, since using(this) will produce nested calls

        """
