def find_filter_class(filtername):
    """
    Lookup a filter by name. Return None if not found.
    """
def get_filter_by_name(filtername, **options):
    """
    Return an instantiated filter.

        Options are passed to the filter initializer if wanted.
        Raise a ClassNotFound if not found.
    
    """
def get_all_filters():
    """
    Return a generator of all filter names.
    """
2021-03-02 20:48:02,950 : INFO : tokenize_signature : --> do i ever get here?
def _replace_special(ttype, value, regex, specialttype,
                     replacefunc=lambda x: x):
    """
    Highlight special code tags in comments and docstrings.

        Options accepted:

        `codetags` : list of strings
           A list of strings that are flagged as code tags.  The default is to
           highlight ``XXX``, ``TODO``, ``BUG`` and ``NOTE``.
    
    """
    def __init__(self, **options):
        """
        'codetags'
        """
    def filter(self, lexer, stream):
        """
        Convert mathematical symbols such as \\<longrightarrow> in Isabelle
            or \\longrightarrow in LaTeX into Unicode characters.

            This is mostly useful for HTML or console output when you want to
            approximate the source rendering you'd see in an IDE.

            Options accepted:

            `lang` : string
               The symbol language. Must be one of ``'isabelle'`` or
               ``'latex'``.  The default is ``'isabelle'``.
    
        """
    def __init__(self, **options):
        """
        'lang'
        """
    def filter(self, lexer, stream):
        """
        Convert keywords to lowercase or uppercase or capitalize them, which
            means first letter uppercase, rest lowercase.

            This can be useful e.g. if you highlight Pascal code and want to adapt the
            code to your styleguide.

            Options accepted:

            `case` : string
               The casing to convert keywords to. Must be one of ``'lower'``,
               ``'upper'`` or ``'capitalize'``.  The default is ``'lower'``.
    
        """
    def __init__(self, **options):
        """
        'case'
        """
    def filter(self, lexer, stream):
        """
        Highlight a normal Name (and Name.*) token with a different token type.

            Example::

                filter = NameHighlightFilter(
                    names=['foo', 'bar', 'baz'],
                    tokentype=Name.Function,
                )

            This would highlight the names "foo", "bar" and "baz"
            as functions. `Name.Function` is the default token type.

            Options accepted:

            `names` : list of strings
              A list of names that should be given the different token type.
              There is no default.
            `tokentype` : TokenType or string
              A token type or a string containing a token type name that is
              used for highlighting the strings in `names`.  The default is
              `Name.Function`.
    
        """
    def __init__(self, **options):
        """
        'names'
        """
    def filter(self, lexer, stream):
        """
        Raise an exception when the lexer generates an error token.

            Options accepted:

            `excclass` : Exception class
              The exception class to raise.
              The default is `pygments.filters.ErrorToken`.

            .. versionadded:: 0.8
    
        """
    def __init__(self, **options):
        """
        'excclass'
        """
    def filter(self, lexer, stream):
        """
        Convert tabs, newlines and/or spaces to visible characters.

            Options accepted:

            `spaces` : string or bool
              If this is a one-character string, spaces will be replaces by this string.
              If it is another true value, spaces will be replaced by ``·`` (unicode
              MIDDLE DOT).  If it is a false value, spaces will not be replaced.  The
              default is ``False``.
            `tabs` : string or bool
              The same as for `spaces`, but the default replacement character is ``»``
              (unicode RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK).  The default value
              is ``False``.  Note: this will not work if the `tabsize` option for the
              lexer is nonzero, as tabs will already have been expanded then.
            `tabsize` : int
              If tabs are to be replaced by this filter (see the `tabs` option), this
              is the total number of characters that a tab should be expanded to.
              The default is ``8``.
            `newlines` : string or bool
              The same as for `spaces`, but the default replacement character is ``¶``
              (unicode PILCROW SIGN).  The default value is ``False``.
            `wstokentype` : bool
              If true, give whitespace the special `Whitespace` token type.  This allows
              styling the visible whitespace differently (e.g. greyed out), but it can
              disrupt background colors.  The default is ``True``.

            .. versionadded:: 0.8
    
        """
    def __init__(self, **options):
        """
        'spaces'
        """
    def filter(self, lexer, stream):
        """
        ' '
        """
            def replacefunc(wschar):
                """
                ' '
                """
def GobbleFilter(Filter):
    """
    Gobbles source code lines (eats initial characters).

        This filter drops the first ``n`` characters off every line of code.  This
        may be useful when the source code fed to the lexer is indented by a fixed
        amount of space that isn't desired in the output.

        Options accepted:

        `n` : int
           The number of characters to gobble.

        .. versionadded:: 1.2
    
    """
    def __init__(self, **options):
        """
        'n'
        """
    def gobble(self, value, left):
        """
        ''
        """
    def filter(self, lexer, stream):
        """
         How many characters left to gobble.
        """
def TokenMergeFilter(Filter):
    """
    Merges consecutive tokens with the same token type in the output
        stream of a lexer.

        .. versionadded:: 1.2
    
    """
    def __init__(self, **options):
        """
        'codetagify'
        """
