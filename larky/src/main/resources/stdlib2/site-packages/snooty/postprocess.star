def get_title_injection_candidate(node: n.Node) -> Optional[n.Parent[n.Node]]:
    """
    Dive into a tree of nodes, and return the deepest non-inline node if and only if the tree is linear.
    """
def get_deepest(node: n.Node) -> Optional[n.Node]:
    """
    Dive into a tree of nodes, and return the deepest node if and only if the tree is linear.
    """
def deep_copy_position(source: n.Node, dest: n.Node) -> None:
    """
    Copy the source position data from one node to another, for the case
           where the dest node's positional data is irrelevant or comes from another file.
    """
def ProgramOptionHandler:
    """
    Handle the program & option rstobjects, using the last program target
           to populate option targets.
    """
    def __init__(self, diagnostics: Dict[FileId, List[Diagnostic]]) -> None:
        """
        f"{node.domain}:{node.name}
        """
def IncludeHandler:
    """
    Iterate over all pages to find include directives. When found, replace their
        `children` property with the contents of the include file.
        Because the include contents are added to the tree on which the event parser is
        running, they will automatically be parsed and have their includes expanded, too.
    """
2021-03-02 20:52:13,064 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,064 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,064 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,065 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,065 : INFO : tokenize_signature : --> do i ever get here?
    def __init__(
        self,
        diagnostics: Dict[FileId, List[Diagnostic]],
        slug_fileid_mapping: Dict[str, FileId],
        pages: Dict[FileId, Page],
    ) -> None:
        """
        Helper function to determine if the given node contains specified start-after or end-before text.

                Note: For now, we are only splicing included files based on Comments and TargetIdentifier nodes.
                Comments have Text nodes as children; Labels have TargetIdentifiers as children.
        """
2021-03-02 20:52:13,065 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,066 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,066 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,066 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,066 : INFO : tokenize_signature : --> do i ever get here?
    def bound_included_AST(
        self,
        nodes: MutableSequence[n.Node],
        start_after_text: Optional[str],
        end_before_text: Optional[str],
    ) -> Tuple[MutableSequence[n.Node], bool, bool]:
        """
        Given an AST in the form of nodes, return a subgraph of that AST by removing nodes 'outside' of
                    the bound formed by the nodes containing the start_after_text or end_before_text. In in-order traversal,
                    a node is considered 'outside' the subgraph if it precedes and is not any ancestor of the start-after node,
                    or if it succeeds and is not any ancestor of the end-before node.
        """
    def __call__(self, fileid_stack: FileIdStack, node: n.Node) -> None:
        """
        Get filename of include
        """
def TabsSelectorHandler:
    """
     Warn if tabs-selector is used without corresponding tabset

    """
    def __call__(self, fileid_stack: FileIdStack, node: n.Node) -> None:
        """
        tabs-pillstrip
        """
def TargetHandler:
    """
     Frankly, this is silly. We just pick the longest identifier. This is arbitrary,
     and we can consider this behavior implementation-defined to be changed later if needed.
     It just needs to be something consistent.

    """
def HeadingHandler:
    """
    Construct a slug-title mapping of all pages in property, and rewrite
           heading IDs so as to be unique.
    """
    def __init__(self, targets: TargetDatabase) -> None:
        """
        f"-{counter}
        """
def Postprocessor:
    """
    Handles all postprocessing operations on parsed AST files.

        The only method that should be called on an instance of Postprocessor is run(). This method
        handles calling all other methods and ensures that parse operations are run in the correct order.
    """
    def __init__(self, project_config: ProjectConfig, targets: TargetDatabase) -> None:
        """
        Run all postprocessing operations and return a dictionary containing the metadata document to be saved.
        """
    def generate_metadata(self) -> n.SerializedNode:
        """
        title
        """
2021-03-02 20:52:13,075 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,075 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,075 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,075 : INFO : tokenize_signature : --> do i ever get here?
    def run_event_parser(
        self,
        node_listeners: Iterable[Tuple[str, Callable[[FileIdStack, n.Node], None]]],
        page_listeners: Iterable[Tuple[str, Callable[[FileIdStack, Page], None]]] = (),
    ) -> None:
        """
        .txt

        """
    def _attach_doc_title(self, fileid_stack: FileIdStack, node: n.RefRole) -> None:
        """
        When a node of type ref_role is encountered, ensure that it references a valid target.

                If so, append the full URL to the AST node. If not, throw an error.
        
        """
2021-03-02 20:52:13,078 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,078 : INFO : tokenize_signature : --> do i ever get here?
    def attempt_disambugation(
        self, fileid: FileId, candidates: Sequence[TargetDatabase.Result]
    ) -> Sequence[TargetDatabase.Result]:
        """
        Given multiple possible targets we can link to, attempt to narrow down the
                   list to one probably-intended target under a set of narrow circumstances.
        """
    def handle_substitutions(self) -> None:
        """
        Find and replace substitutions throughout project
        """
    def replace_substitutions(self, fileid_stack: FileIdStack, node: n.Node) -> None:
        """
        When a substitution is defined, add it to the page's index.

                When a substitution is referenced, populate its children if possible.
                If not, save this node to be populated at the end of the page.
        
        """
    def finalize_substitutions(self, fileid_stack: FileIdStack, page: Page) -> None:
        """
        Attempt to populate any yet-unresolved substitutions (substitutions defined after usage) .

                Clear definitions and unreplaced nodes for the next page.
        
        """
    def reset_seen_definitions(self, fileid_stack: FileIdStack, node: n.Node) -> None:
        """
        std
        """
    def build_slug_fileid_mapping(self) -> None:
        """
        Construct a {slug: fileid} mapping so that we can retrieve the full file name
                given a slug. We cannot use the with_suffix method since the type of the slug
                in find_toctree_nodes(...) is string rather than FileId.
        """
    def build_toctree(self) -> Dict[str, SerializableType]:
        """
        Build property toctree
        """
2021-03-02 20:52:13,082 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,082 : INFO : tokenize_signature : --> do i ever get here?
    def find_toctree_nodes(
        self, fileid: FileId, ast: n.Node, node: Dict[str, Any]
    ) -> None:
        """
        Iterate over AST to find toctree directives and construct their nodes for the unified toctree
        """
    def breadcrumbs(self) -> Dict[str, List[str]]:
        """
        Generate breadcrumbs for each page represented in the toctree
        """
    def toctree_order(self) -> List[str]:
        """
        Return a pre-order traversal of the toctree to be used for internal page navigation
        """
def pre_order(node: Dict[str, Any], order: List[str]) -> None:
    """
    slug
    """
def get_paths(node: Dict[str, Any], path: List[str], all_paths: List[Any]) -> None:
    """
    Helper function used to retrieve the breadcrumbs for a particular slug
    """
def clean_slug(slug: str) -> str:
    """
    Strip file extension and leading/trailing slashes (/) from string
    """
def DevhubPostprocessor(Postprocessor):
    """
    Postprocess operation to be run if a project's default_domain is equal to 'devhub'
    """
2021-03-02 20:52:13,086 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:13,086 : INFO : tokenize_signature : --> do i ever get here?
    def run(
        self, pages: Dict[FileId, Page]
    ) -> Tuple[Dict[str, SerializableType], Dict[FileId, List[Diagnostic]]]:
        """
        Clean a slug and validate that it is a known page. If it is not, return None.
        """
    def reset_query_fields(self, fileid_stack: FileIdStack, page: Page) -> None:
        """
        To be called at the start of each page: reset the query field dictionary
        """
    def append_query_fields(self, fileid_stack: FileIdStack, page: Page) -> None:
        """
        To be called at the end of each page: append the query field dictionary to the
                top level of the page's class instance.
        
        """
    def flatten_devhub_article(self, fileid_stack: FileIdStack, node: n.Node) -> None:
        """
        Extract fields from a page's AST and expose them as a queryable nested document in the page document.
        """
