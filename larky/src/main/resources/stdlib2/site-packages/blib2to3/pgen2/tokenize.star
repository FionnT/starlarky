def group(*choices):
    """
    (
    """
def any(*choices):
    """
    *
    """
def maybe(*choices):
    """
    ?
    """
def _combinations(*l):
    """

    """
def TokenError(Exception):
    """
     for testing
    """
def tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:
    """

        The tokenize() function accepts two parameters: one representing the
        input stream, and one providing an output mechanism for tokenize().

        The first parameter, readline, must be a callable object which provides
        the same interface as the readline() method of built-in file objects.
        Each call to the function should return one line of input as a string.

        The second parameter, tokeneater, must also be a callable object. It is
        called once for each token, with five arguments, corresponding to the
        tuples generated by generate_tokens().
    
    """
def tokenize_loop(readline, tokeneater):
    """
 
    """
    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:
        """

        """
    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:
        """
 
        """
def _get_normal_name(orig_enc: str) -> str:
    """
    Imitates get_normal_name in tokenizer.c.
    """
def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:
    """

        The detect_encoding() function is used to detect the encoding that should
        be used to decode a Python source file. It requires one argument, readline,
        in the same way as the tokenize() generator.

        It will call readline a maximum of twice, and return the encoding used
        (as a string) and a list of any lines (left as bytes) it has read
        in.

        It detects the encoding from the presence of a utf-8 bom or an encoding
        cookie as specified in pep-0263. If both a bom and a cookie are present, but
        disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
        charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
        'utf-8-sig' is returned.

        If no encoding is specified, then the default of 'utf-8' will be returned.
    
    """
    def read_or_stop() -> bytes:
        """
        ascii
        """
def untokenize(iterable: Iterable[TokenInfo]) -> Text:
    """
    Transform tokens back into Python source code.

        Each element returned by the iterable must be a token sequence
        with at least two elements, a token number and token value.  If
        only two tokens are passed, the resulting output is poor.

        Round-trip invariant for full input:
            Untokenized source will match input source exactly

        Round-trip invariant for limited input:
            # Output text will tokenize the back to the input
            t1 = [tok[:2] for tok in generate_tokens(f.readline)]
            newcode = untokenize(t1)
            readline = iter(newcode.splitlines(1)).next
            t2 = [tok[:2] for tokin generate_tokens(readline)]
            assert t1 == t2
    
    """
2021-03-02 20:52:07,927 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:52:07,927 : INFO : tokenize_signature : --> do i ever get here?
def generate_tokens(
    readline: Callable[[], Text], grammar: Optional[Grammar] = None
) -> Iterator[GoodTokenInfo]:
    """

        The generate_tokens() generator requires one argument, readline, which
        must be a callable object which provides the same interface as the
        readline() method of built-in file objects. Each call to the function
        should return one line of input as a string.  Alternately, readline
        can be a callable function terminating with StopIteration:
            readline = open(myfile).next    # Example of alternate readline

        The generator produces 5-tuples with these members: the token type; the
        token string; a 2-tuple (srow, scol) of ints specifying the row and
        column where the token begins in the source; a 2-tuple (erow, ecol) of
        ints specifying the row and column where the token ends in the source;
        and the line on which the token was found. The line passed is the
        logical line; continuation lines are included.
    
    """
