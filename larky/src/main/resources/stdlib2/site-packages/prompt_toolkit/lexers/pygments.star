def SyntaxSync(metadef=ABCMeta):
    """

        Syntax synchroniser. This is a tool that finds a start position for the
        lexer. This is especially important when editing big documents; we don't
        want to start the highlighting by running the lexer from the beginning of
        the file. That is very slow when editing.
    
    """
2021-03-02 20:51:32,475 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,475 : INFO : tokenize_signature : --> do i ever get here?
    def get_sync_start_position(
        self, document: Document, lineno: int
    ) -> Tuple[int, int]:
        """

                Return the position from where we can start lexing as a (row, column)
                tuple.

                :param document: `Document` instance that contains all the lines.
                :param lineno: The line that we want to highlight. (We need to return
                    this line, or an earlier position.)
        
        """
def SyncFromStart(SyntaxSync):
    """

        Always start the syntax highlighting from the beginning.
    
    """
2021-03-02 20:51:32,475 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,475 : INFO : tokenize_signature : --> do i ever get here?
    def get_sync_start_position(
        self, document: Document, lineno: int
    ) -> Tuple[int, int]:
        """

            Synchronize by starting at a line that matches the given regex pattern.
    
        """
    def __init__(self, pattern: str) -> None:
        """

                Scan backwards, and find a possible position to start.
        
        """
    def from_pygments_lexer_cls(cls, lexer_cls: "PygmentsLexerCls") -> "RegexSync":
        """

                Create a :class:`.RegexSync` instance for this Pygments lexer class.
        
        """
def _TokenCache(Dict[Tuple[str, ...], str]):
    """

        Cache that converts Pygments tokens into `prompt_toolkit` style objects.

        ``Token.A.B.C`` will be converted into:
        ``class:pygments,pygments.A,pygments.A.B,pygments.A.B.C``
    
    """
    def __missing__(self, key: Tuple[str, ...]) -> str:
        """
        class:
        """
def PygmentsLexer(Lexer):
    """

        Lexer that calls a pygments lexer.

        Example::

            from pygments.lexers.html import HtmlLexer
            lexer = PygmentsLexer(HtmlLexer)

        Note: Don't forget to also load a Pygments compatible style. E.g.::

            from prompt_toolkit.styles.from_pygments import style_from_pygments_cls
            from pygments.styles import get_style_by_name
            style = style_from_pygments_cls(get_style_by_name('monokai'))

        :param pygments_lexer_cls: A `Lexer` from Pygments.
        :param sync_from_start: Start lexing at the start of the document. This
            will always give the best results, but it will be slow for bigger
            documents. (When the last part of the document is display, then the
            whole document will be lexed by Pygments on every key stroke.) It is
            recommended to disable this for inputs that are expected to be more
            than 1,000 lines.
        :param syntax_sync: `SyntaxSync` object.
    
    """
2021-03-02 20:51:32,477 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,477 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,477 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,477 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,477 : INFO : tokenize_signature : --> do i ever get here?
    def __init__(
        self,
        pygments_lexer_cls: Type["PygmentsLexerCls"],
        sync_from_start: FilterOrBool = True,
        syntax_sync: Optional[SyntaxSync] = None,
    ) -> None:
        """
         Instantiate the Pygments lexer.

        """
2021-03-02 20:51:32,478 : INFO : tokenize_signature : --> do i ever get here?
2021-03-02 20:51:32,478 : INFO : tokenize_signature : --> do i ever get here?
    def from_filename(
        cls, filename: str, sync_from_start: FilterOrBool = True
    ) -> "Lexer":
        """

                Create a `Lexer` from a filename.
        
        """
    def lex_document(self, document: Document) -> Callable[[int], StyleAndTextTuples]:
        """

                Create a lexer function that takes a line number and returns the list
                of (style_str, text) tuples as the Pygments lexer returns for that line.
        
        """
        def get_syntax_sync() -> SyntaxSync:
            """
             The Syntax synchronisation object that we currently use. 
            """
        def find_closest_generator(i: int) -> Optional[LineGenerator]:
            """
             Return a generator close to line 'i', or None if none was found. 
            """
        def create_line_generator(start_lineno: int, column: int = 0) -> LineGenerator:
            """

                        Create a generator that yields the lexed lines.
                        Each iteration it yields a (line_number, [(style_str, text), ...]) tuple.
            
            """
            def get_text_fragments() -> Iterable[Tuple[str, str]]:
                """
                \n
                """
        def get_generator(i: int) -> LineGenerator:
            """

                        Find an already started generator that is close, or create a new one.
            
            """
        def get_line(i: int) -> StyleAndTextTuples:
            """
             Return the tokens for a given line number. 
            """
